{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_path = input(\"Insert path to the Model: \\t\")\n",
    "crop_top = int(input(\"Insert model crop top value: \\t\"))\n",
    "tub_path = input(\"Insert path to the tub of the track: \\t\")\n",
    "tub_path = tub_path + '/' if tub_path[-1] != '/' else tub_path\n",
    "\n",
    "while not os.path.isdir(tub_path) or not os.path.isfile(model_path):\n",
    "    print(\"One of the paths was incorrect\")\n",
    "\n",
    "    model_path = input(\"Insert path to the Model: \\t\")\n",
    "    tub_path = input(\"Insert path to the tub of the track: \\t\")\n",
    "    tub_path = tub_path + '/' if tub_path[-1] != '/' else tub_path\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, put paths and crop manually\n",
    "model_path = \"/Users/brianpulfer/mycar/thesis/models/sim/tests/cropping/crop120.h5\"\n",
    "tub_path = \"/Users/brianpulfer/mycar/thesis/data/tub320x240\"\n",
    "tub_path = tub_path + '/' if tub_path[-1] != '/' else tub_path\n",
    "crop_top = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected images size:  (320, 240)\n",
      "Image size after top crop:  (320, 120)\n",
      "Detected model input size:  (320, 120)\n",
      "Sizes seem to match\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "img_in (InputLayer)             (None, 120, 320, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 58, 158, 24)  1824        img_in[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 58, 158, 24)  0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 27, 77, 32)   19232       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 27, 77, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 12, 37, 64)   51264       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 12, 37, 64)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 10, 35, 64)   36928       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 10, 35, 64)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 8, 33, 64)    36928       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 8, 33, 64)    0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flattened (Flatten)             (None, 16896)        0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          1689700     flattened[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 100)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           5050        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 50)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "n_outputs0 (Dense)              (None, 1)            51          dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "n_outputs1 (Dense)              (None, 1)            51          dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,841,028\n",
      "Trainable params: 1,841,028\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Loading model\n",
    "model = keras.models.load_model(model_path)\n",
    "\n",
    "# Getting model input size\n",
    "MODEL_INPUT_SIZE = model.layers[0]._batch_input_shape[1:3]\n",
    "MODEL_INPUT_SIZE = (MODEL_INPUT_SIZE[1], MODEL_INPUT_SIZE[0])\n",
    "\n",
    "# Getting dataset image size\n",
    "images_names = [name for name in sorted(os.listdir(tub_path)) if 'jpg' in name.lower()]\n",
    "first_image = Image.open(tub_path + images_names[0])\n",
    "\n",
    "w, h = (first_image.size[0], first_image.size[1])\n",
    "print(\"Detected images size: \", first_image.size)\n",
    "print(\"Image size after top crop: \", (w, h-crop_top))\n",
    "print(\"Detected model input size: \", MODEL_INPUT_SIZE)\n",
    "\n",
    "if MODEL_INPUT_SIZE != (w, h-crop_top):\n",
    "    print(\"WARNING: Input sizes do not match! Reshaping might yield bad predictions\")\n",
    "else:\n",
    "    print(\"Sizes seem to match\\n\")\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3657/3657 [00:05<00:00, 641.59it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_dataset(tub_path, w, h, crop_top, dave2=False):\n",
    "    # Collecting dataset in a tensor\n",
    "    n = len([i for i in os.listdir(tub_path) if 'jpg' in i.lower()])\n",
    "    X, Y = [], []\n",
    "    for image_name in tqdm(images_names):\n",
    "        if 'jpg' not in image_name.lower():\n",
    "            continue\n",
    "\n",
    "        # Defining paths\n",
    "        image_path = os.path.join(tub_path, image_name)\n",
    "        record_path = os.path.join(tub_path, \"record_\" + image_name.split(\"_\")[0] + \".json\")\n",
    "\n",
    "        # Loading image and relative record\n",
    "        image = Image.open(image_path)\n",
    "        resized = image.resize((w, h))\n",
    "        normalized = np.array(resized) / 255.0\n",
    "        cropped = normalized[crop_top:, ...]\n",
    "\n",
    "        if dave2:\n",
    "            cropped = cv2.cvtColor((cropped*255).astype('uint8'), cv2.COLOR_RGB2YUV)\n",
    "            cropped = cropped.astype('float') / 255\n",
    "\n",
    "        record = json.load(open(record_path))\n",
    "\n",
    "        # Loading labels\n",
    "        angle = record['user/angle']\n",
    "        throttle = record['user/throttle']\n",
    "\n",
    "        # Adding data in the tensor\n",
    "        X.append(np.array(cropped).astype(np.float32))\n",
    "        Y.append(np.array([angle, throttle]))\n",
    "\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "X, Y = get_dataset(tub_path, w, h, crop_top, dave2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predictions\n",
    "Y_hat = model.predict(X)\n",
    "angle_predictions = Y_hat[0]\n",
    "throttle_predictions = Y_hat[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering MSE: 0.08794156657014457\t\tThrottle MSE: 0.0032620402743986842\n",
      "Steering MAE: 0.20881679831433678\t\tThrottle MAE: 0.03461342960450801\n",
      "Total LOSS: 0.09120360684454325\n"
     ]
    }
   ],
   "source": [
    "# Evaluating model offline\n",
    "mse_steer, mse_throttle = 0, 0\n",
    "mae_steer, mae_throttle = 0, 0\n",
    "for i in range(len(X)):\n",
    "    s, t = Y[i][0], Y[i][1]\n",
    "    s_hat, t_hat = angle_predictions[i][0], throttle_predictions[i][0]\n",
    "\n",
    "    mse_steer += (s_hat - s)**2\n",
    "    mse_throttle += (t_hat - t)**2\n",
    "\n",
    "    mae_steer += abs(s_hat - s)\n",
    "    mae_throttle += abs(t_hat -t)\n",
    "\n",
    "# Getting average\n",
    "mse_steer /= len(X)\n",
    "mse_throttle /= len(X)\n",
    "mae_steer /= len(X)\n",
    "mae_throttle /= len(X)\n",
    "\n",
    "# Getting total loss\n",
    "loss = mse_steer + mse_throttle\n",
    "\n",
    "# Printing\n",
    "print(\"Steering MSE: {}\\t\\tThrottle MSE: {}\".format(mse_steer, mse_throttle))\n",
    "print(\"Steering MAE: {}\\t\\tThrottle MAE: {}\".format(mae_steer, mae_throttle))\n",
    "print(\"Total LOSS: {}\".format(loss))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}